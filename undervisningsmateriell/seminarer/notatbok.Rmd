---
title: "STV2022 -- Store tekstdata"
date: "`r Sys.Date()`"
author: Solveig Bjørkholt og Martin Søyland
output:
  rmdformats::downcute:
    highlight: 
    self_contained: true
    default_style: "dark"
    downcute_theme: "chaos"
    number_sections: true
    code_folding: show
    toc_depth: 4
    use_bookdown: true
pkgdown:
  as_is: true
bibliography: stv2022.bib
csl: american-political-science-association.csl
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=FALSE, message=FALSE, error=FALSE}

# set.wd("./undervisningsmateriell/seminarer")
library(knitr)
library(formatR)
library(rmdformats)

## Global options
# options(max.print = "75")
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, highlight = TRUE
)
#opts_knit$set(width = 75)

library(stringr)
library(dplyr) 

# if("tidytext" %in% installed.packages() == FALSE) install.packages("tidytext")

```

# Introduksjon {#introduksjon}

Velkommen til STV2022 -- Store teksdata!

Dette er en arbeidsbok som går gjennom de forskjellige delene i kurset, med tilhørende R-kode. Meningen med arbeidsboken, er at den kan brukes som forslag til implementering av metoder i semesteroppgaven. Merk likevel at dette ikke er en fasit!

## Kort om kurset

I kurset skal vi bli kjent med analyseprosessen av store tekstdata: Hvordan samler man effektivt og redelig store mengder politiske tekster? Hva må til for å gjøre slike tekster klare for analyse? Og hvordan kan vi analysere tekstene?

Politikere og politiske partier produserer store mengder tekst hver dag. Om det er gjennom debatter, taler på Stortinget, lovforslag fra regjeringen, høringer, offentlige utredninger med mer, er digitaliserte politiske tekster i det offentlige blitt mer tilgjengelig de siste tiårene. Dette har åpnet et mulighetsrom for tekstanalyse som ikke var mulig/veldig vanskelig og tidkrevende før.

Det kan ofte være vanskelig å finne mønster som kan svare på spørsmål og teorier vi har i statsvitenskap i disse store tekstsamlingene. Derfor kan vi se til metoder innenfor maskinlæring for å analysere store samlinger av tekst systematisk. Samtidig er ikke alltid digitaliserte politiske tekster tilrettelagt for å analysers direkte. I disse tilfellene er god strukturering av rådata viktig.

Gjennom å delta i dette kurset vil du lære å søke i store mengder dokumenter, oppsummere disse på meningsfulle måter og indentifisere riktige analysemetoder for å teste statsvitenskaplige teorier med store tekstdata. Kurset vil dekke samling av store volum tekst fra offentlige kilder, strukturering og klargjøring av tekst for analyse og kvantitative tekstanalysemetoder.

## Oppbygging av arbeidsboken

Denne arbeidsboken er ment som supplement til pensum i kurset forøvrig. Her vil vi gå gjennom de ulike delene av kurset, og spesielt legge oss tett opp til seminarundervisningen.

Under vil vi gå gjennom undervisningsopplegget, som arbeidsboken er lagt opp etter. Delene av boken er strukturert som følgende:

1.  [Anskaffelse av tekst](#anskaff)
2.  [Laste inn eksisterende tekstkilder](#lastetekst)
3.  [Forbehandling av tekst (preprosessering)](#prepros)
4.  [Veiledet læring (supervised)](#sup)
5.  [Ikke-veiledet læring (unsupervised)](#unsup)
6.  [Ordbøker](#ordboker)
7.  [Tekstsatistikk](#tekststats)
8.  [Sentiment](#sentiment)
9.  [Temamodellering](#topicmod)
10. [Latente posisjoner i tekst](#posisjon)


## Anbefalte forberedelser

Siden kurset krever noe forkunnskap om R og generell metodisk kompetanse, anbefaler vi å se over følgende materiale før kurset starter:

- [Arbeidsbøker for R ved UiO](https://shinyibv02.uio.no/connect/#/apps/55/access)
- [R materiale for STV1020](https://github.com/liserodland/STV1020)



# Undervisning

Undervisningen i STV2022 består av 10 forelesninger og 5 seminarer. Vi vil bruke forelesningene til å oppsummere hovedkonseptene i hver ukes tema, både metodisk og anvendt. Seminarene vil ha hovedfokus på teknisk gjennomføring av tekstanalyse i R. Hvert seminar vil være delt i to med én del der seminarleder går gjennom ekstempler på kodeimplementering og én del der studentene kan jobbe med semesteroppgaven. Det er også verdt å merke seg at mange av implementeringene i kurset krever en del prøving og feiling.

Merk at det etter hvert seminar skal leveres inn et utkast av oppgaven for temaet man har gått gjennom i seminaret. Disse delene må bestås for å få vurdert semesteroppgave.

## Forelesninger

De ti forelesningene har følgende timeplan (høsten 2022):

| Dato         | Tid         | Aktivitet                          | Sted       | Foreleser                  | Ressurser/pensum                                                                        |
|:-------------|:------------|:-----------------------------------|:-----------|:---------------------------|:----------------------------------------------------------------------------------------|
| ti. 23. aug. | 10:15–12:00 | Introduksjon                       |	ES, Aud. 5 | S. Bjørkholt og M. Søyland | @Grimmer2022 kap. 1-2 og 22, @Lucas2015, @Silge2017 kap. 1, @Pang2008 kap. 1            |
| ti. 30. aug. | 10:15–12:00 | Anskaffelse og innlasting av tekst | ES, Aud. 5 | M. Søyland                 | @Grimmer2022 kap. 3-4, @Cooksey2014 kap. 1, @Wickham2020, @Hoyland2019                  |
| ti. 6.  sep. | 10:15–12:00 | Forbehandling av tekst 1           | ES, Aud. 5 | M. Søyland                 | @Grimmer2022 kap. 5, @Silge2017 kap. 3, @Joergensen2019, @Barnes2019, @Benoit2020       |
| ti. 13. sep. | 10:15–12:00 | Forbehandling av tekst 2           | ES, Aud. 5 | S. Bjørkholt               | @Grimmer2022 kap. 9, @Silge2017 kap. 4, @Denny2018                                      |
| ti. 20. sep. | 10:15–12:00 | Bruke API – Case: Stortinget	      | ES, Aud. 5 | M. Søyland                 | @datastortinget2022, @Soeyland2022, @Finseraas2021                                      |
| ti. 11. okt. | 10:15–12:00 | Veiledet og ikke-veiledet læring   | ES, Aud. 5 | S. Bjørkholt               | @Grimmer2022 kap. 10 og 17, @DOrazio2014, @Feldman2006a, @Feldman2006b  @Muchlinski2016 |
| ti. 18. okt. | 10:15–12:00 | Ordbøker, tekstlikhet og sentiment | ES, Aud. 5 | S. Bjørkholt               | @Grimmer2022 kap. 7 og 16, @Silge2017 kap. 2, @Pang2008 kap. 3-4, @Liu2015, Liu2015a    | 
| ti. 25. okt. | 10:15–12:00 | Temamodellering	                  | ES, Aud. 5 | M. Søyland                 | @Grimmer2022 kap. 13, @Blei2012, @Silge2017 kap. 6, @Roberts2014                        |
| ti. 1. nov.	 | 10:15–12:00 | Estimere latent posisjon fra tekst | ES, Aud. 5 | S. Bjørkholt               | @Laver2003, @Slapin2008, @Lowe2017, @Lauderdale2016, @Peterson2018                      |
| ti. 15. nov. | 10:15–12:00 | Oppsummering	                      | ES, Aud. 5 | S. Bjørkholt og M. Søyland | @Grimmer2022 kap 28, @Wilkerson2017                                                     |


## Seminarer

| Uke | Aktivitet                                          | 
|:----|:---------------------------------------------------|
| 36  | Seminar 1: Anskaffe tekst og lage dtm i R          |
| 38  | Seminar 2: Preprosessering av tekstdata i R        |
| 42  | Seminar 3: Veiledet og ikke-veiledet læring i R    |
| 44  | Seminar 4: Modelleringsmetoder i R                 |
| 46  | Seminar 5: Fra tekst til funn, Q&A og oppgavehjelp |

Seminarledere:

- Eli Sofie Baltzersen <elibal@student.sv.uio.no>
- Eric Gabo Ekeberg Nilsen <e.g.e.nilsen@stv.uio.no>

## Pensum

Som med alle andre fag, er det sterkt anbefalt at man ser over pensum før forelesning og seminar. Likevel kan pensum i kurset til tider være noe teknisk og uhåndterbart. Det er ikke forventet å _pugge_ formler eller fult ut forstå de matematiske beregninger bak de forskjellige modelleringsmetodene (selv om det åpenbart kan gjøre stoffet lettere å forstå). Hovedfokuset vårt vil være på å forstå hvilke operasjoner man må gjøre for å gå fra tekst til funn, hvilke antagelser man gjør i prosessen og klare å velge de riktige modellene for spørsmålet man vil ha svar på.

Grunnboken i pensum er @Grimmer2022. Vi vil lene oss mye på denne over alle temaene vi gjennomgår. For R har vi valgt å gjøre materialet så standardisert som mulig ved å bruke `tidyverse` så langt det lar seg gjøre. Spesielt bruker vi @Silge2017 for implementeringer via R-pakken `tidytext`.

Vi har også lagt inn noen bidrag som anvender metodene vi går gjennom i løpet av kurset, som @Peterson2018, @Lauderdale2016, @Hoyland2019, @Finseraas2021, for å synliggjøre nytten av metodene i anvendt forskning.


# Laste inn tekstdata {#lastetekst}

I denne delen av arbeidsboken vil vi gå gjennom noen eksempler på hvordan vi kan laste inn tekstdata i R.

Tekstdata kan komme i uendelig mange forskjellige formater, og det er umulig å gå gjennom alle. Vi har likevel noen typer data som er mer vanlig innenfor statsvitenskap enn andre. Under vil vi gå gjennom 1) lasting av ulike to-dimensjonale datasett (.rda/.Rdata, .csv, .sav og .dta), 2) rå tekstfiler (.txt), 3) tekstfiler med overhead (.pdf og .docx).

## To-dimensjonale datasett

Det vanligste formatet på eksisterende data innenfor politisk analyse er to-dimensjonale datasett. Et datasett består av rader (vanligvis observasjoner/enheter) og kolonner (vanligvis variabler). Disse datasettene kommer i mange forskjellige format, men de aller fleste (eller alle) kan leses inn i R om man finner de rette funksjonene.

Under vil vi illustre de forskjellige måtene å laste inn data på med eksempeldata fra pakken `stortingscrape`, som inneholder meta data på alle saker Stortinget behandlet i 2019-2020-sesjonen:

```{r laste_data_data}

saker <- stortingscrape::cases$root

saker %>% 
  select(id, document_group, status, title_short) %>% 
  mutate(title_short = str_sub(title_short, 1, 30)) %>% 
  tail()

```

### .rda og .Rdata

R har sin egen type filformat med filtypene `.rda` og `.Rdata` (`.Rds` finnes også, men vi hopper over det her). Disse to formatene er faktisk akkurat det samme formatet; `.rda` er bare en forkortelse for `.Rdata`. Disse filene er komprimerte versjoner av objekter i *Environment*, som man kan lagre lokalt. Fordi denne filtypen har veldig god kompresjon og selvfølgelig virker sømløst sammen med R, er det et veldig nyttig format å bruke. Dette gjelder særlig når man jobber med store tekstdata. 

Som eksempel på lagring kan jeg trekke ut data fra `stortingscrape`-pakken og lagre disse lokalt med `save()`-funksjonen:

```{r rda_save, eval=FALSE}

save(saker, file = "./data/saker.rda")

```

Om man har flere objekter i *Environment* man vil lagre samtidig som `.rda / .Rdata`, er dette mulig å gjøre med funksjonen `save.image()`.

For å laste inn `.rda / .Rdata` bruker man funksjonen `load()`:

```{r rda_load}

load("./data/saker.rda")

```

En ting som ofte er litt forvirrende, er at filnavnet til `.rda` ikke nødvendigvis samsvarer med navnet man får opp på objektene i R; objektene i *Environment* vil alltid ha samme navn som de hadde i *Environment* når filen ble lagret.

### .csv

Et veldig enkelt og vanlig format for å distribuere data, er kommaseparerte filer (`.csv`). Man kan enkelt lese inn `.csv`-filer med `read.csv()`, eller, som vist under, med funksjonen `read_csv()` fra pakken `readr`.^[Vi bruker `readr` fordi den virker godt sammen med `tidyverse` og er noe raskere enn base-funksjonen `read.csv()`]

```{r csv}

library(readr)

saker <- read_csv("./data/saker.csv", show_col_types = FALSE)

```

Argumentet `show_col_types` fjerner en beskjed om hvordan data blir lastet inn. Dette kan noen ganger være nyttig å se dette, men det blir fort litt *clutter* av det.

### .sav

### .dta

## Rå tekstfiler (.txt)

## Tekstfiler med overhead

En `.txt`-fil er som den er; det er ingen sjulte datakilder i slike filer. Det er det derimot i andre filformater. En MS Word-fil, for eksempel, er egentlig bare et komprimert arkiv (.zip) med underliggende `html / xml` som bestemmer hvordan filen skal se ut når du åpner den i MS Word. Vi bruker det siste MS Word-dokumentet Martin skrev (bacheloroppgave fra 2013) som eksempel:

```{r msw_zip}

unzip("data/ba_thesis.docx", exdir = "data/wordfiles")

list.files("data/wordfiles/")

```

Dette gjør at disse filene er mye vanskeligere å lese inn i R enn rå tekstfiler, og vi får veldig rar output når vi bruker `readLines()`:

```{r read_msw_feil, warning=TRUE}

readLines("./data/ba_thesis.docx", n = 2)


```

Derfor vil det kreve andre metoder for å lese inn filer med overhead. Under eksemplifiserer vi med `.docx` og `.pdf`, som er de mest brukte av denne type filer.

### .docx

Heldigvis finnes det løsninger for dette også. Her viser vi hvordan vi gjør det med pakken `textreadr`, fordi den har funksjoner for å lese det meste (.doc, .docx, .pdf, .odt, .pptx, osv):

```{r docx}

library(textreadr)

ba_docx <- read_docx("./data/ba_thesis.docx")

ba_docx[43:46]

```

Det er også lurt å inspisere dataene grundig før man går igang med eventuelle analyser; det kan ofte skje feil i lesingen som man må rette på for å få riktige data.

### .pdf

Det samme gjelder for `.pdf`-filer:

```{r read_pdf}

ba_pdf <- read_pdf("./data/ba_thesis.pdf")

ba_pdf <- ba_pdf$text[4] %>% 
  strsplit("\\n") %>% 
  unlist()

ba_pdf[11:14]

```

Her ble outputen av `read_pdf()` delt inn i sider, i tillegg til at teksten ikke ble delt opp i linjer. Så vi har gått inn og tatt ut side 4, delt opp teksten i linjer og trukket ut tilsvarende linjer som vi gjorde i MS Word-filen.

La oss også nevne at endel (spesielt historiske) dokumenter i `.pdf`-format er scannet og bare inneholder bilder av tekst -- ikke tekst man enkelt kan ta ut av dokumentet. Da må man ty til *Optical Character Recognition* (OCR), noe vi dessverre ikke kommer til å gå gjennom i dette kurset.


# Anskaffelse av tekst {#anskaff}

## .html-skraping

Dette kan være lite gøy

## .xml-skraping

Dette er enklere enn html

## .json-skraping

Dette liker jeg ikke. Veldig rar datastrukturering

## Curl

Ja, må vi egentlig snakke om curl

## APIer

Kanskje bruke Stortinget som eksempel.




# Preprosessering {#prepros}

Preprosessering er ganske viktig.

## Sekk med ord

*Alle* tekster er unike!

Ta for eksempel spor 6 på No.4-albumet vi allerede har jobbet med -- *Regndans i skinnjakke*. Hvis vi skal følge en vanlig antagelse i kvantitativ tekstanalyse -- "sekk med ord" eller *bag of words* -- skal vi kunne forstå innholdet i en tekst hvis vi deler opp teksten i segmenter, putter det i en pose, rister posen og tømmer det på et bord. Da vil denne sangen for eksempel se slik ut:

```{r bow, tidy=TRUE, results='markup'}

regndans <- readLines("./data/regndans.txt")

bow <- regndans %>% str_split("\\s") %>% unlist()

set.seed(984301)

cat(bow[sample(1:length(bow))])

```

De fleste (som ikke kan sangen fra før) vil ha vanskelig å forstå hva den egentlig handler om bare ved å se på dette. Vi kan identifisere meningsbærende ord som "Oslofjorden", "Grille", "trampoline", "dragepust", med mer. Likevel er det vanskelig å skjønne hva låtskriveren egentlig vil formidle med denne teksten. Det er dette som gjør "sekk med ord"-antagelsen veldig streng. Språk er veldig komplekst og ordene i en tekst kan endre mening drastisk bare ved å se på en liten del av konteksten de dukker opp i. Om vi bare ser på linjen som inneholder orded "dragepust", innser vi fort at konteksten rundt ordet gir oss et veldig tydelig bilde av hva låtskriveren mener med akkurat den linjen:

```{r dragepust}

regndans[which(str_detect(regndans, "dragepust"))]

```

Likevel gir det oss ikke et godt bilde på hva teksten handler om i sin helhet. Det får vi bare sett ved å se på hele teksten:

```{r regndans_full, echo=FALSE}
cat(paste(regndans, collapse = "\n"))
```

Nå teksten gir mening! Tolkninger kan selvfølgelig variere fra individ til individ og den "riktige" tolkningen, er det bare forfatteren som vet hva er. Personlig tolker jeg denne teksten som et utløp for frustrasjon under corona-pandemien, og prospektene ved livet når samfunnet gjenåpnes, fordi jeg hørte den for første gang under nedstengningen.

Hovedpoenget med å vise dette er at *sekk med ord*-antagelsen er veldig streng og ofte veldig urealistisk. Tekster (og språk generelt) er ekstremt komplekst. Det kan variere mellom geografiske områder (nasjoner, dialekter, osv), aldersgrupper, arenaer (talestol, dialog, monolog, osv), og individuell stil. Oppi alt dette skal vi prøve å finne mønster som sier noe om likhet/ulikhet mellom tekster. Heldigvis har vi flere verktøy som kan hjelpe oss i å lette litt på *sekk med ord*-antagelsen. Men antagelsen vil likevel alltid være der, i en eller annen form. La oss se litt på hvilke teknikker vi kan bruke for å gjøre modellering av tekst noe mer omgripelig¸ men aller først skal vi se litt på hvilke trekk som muligens ikke gir oss så mye informasjon om det vi er ute etter, eller støy, som vi ofte vil fjerne.

<!-- mange gifte par vs. gifte et par -->

## Fjerne trekk?


<!-- ```{r} -->
<!-- library(tidytext) -->

<!-- load("./data/no4.rda") -->

<!-- no4_tokens <- no4 %>% -->
<!--   group_by(spor, titler) %>%  -->
<!--   unnest_tokens(output = token, -->
<!--                 input = tekst) %>%  -->
<!--   count(token) -->

<!-- no4_tokens %>% slice_max(n, n = 1, with_ties = FALSE) -->

<!-- ``` -->



### Punktsetting

### Stoppord

## Rotform av ord

### Stemming

### Lemmatisering

## ngrams

## Taledeler (parts of speech)


# Veildedet læring {#sup}

# Ikke-veiledet læring {#unsup}

# Ordbøker {#ordboker}

# Tekststatistikk {#tekststats}

## Likhet

## Avstand

## Lesbarhet

## Uttrykk

# Sentiment {#sentiment}

## NorSentLex

# Temamodellering {#topicmod}

# Latente posisjoner {#posisjon}

# Oppsummering {#oppsummering}

`r if (knitr::is_html_output()) '# Referanser'`

```{r writeManifest, echo=FALSE, eval=FALSE}
installed.packages()
# Oppdaterer manifest -- jeg tror vi må kjøre denne manuelt hver
# gang før vi pusher til github. uio-serveren er hvertfall ikke fornøyd
# når jeg ikke gjør det.
rsconnect::writeManifest("./")

# For å trekke ut koden fra hele boka til en R-fil
knitr::purl("./notatbok.Rmd",
            output = "./notatbok_kode.R",
            documentation = 1)
```
