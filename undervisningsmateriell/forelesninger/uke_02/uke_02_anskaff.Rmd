---
title: " "
date: '`r format(Sys.time(), "%d-%m-%Y")`'
output:
  revealjs::revealjs_presentation:
    css: style.css
    highlight: breezedark
    incremental: yes
    code_folding: show
    transition: convex
    self_contained: false
    reveal_plugins: ["chalkboard", "notes", "menu", "zoom"]
    reveal_options:
      slideNumber: true
css: style.css
bibliography: ../../../referanser/stv2022.bib
csl: american-political-science-association.csl
editor_options: 
  chunk_output_type: console

---

<font size=12>STV2022 -- Store tekstdata</font></br></br>
<p style='font-size:10;color:#D13F11'>[02] Anskaffelse og innlasting av tekst</p></br>
![](uio_logo.png){width=50%} 

Martin Søyland 
<font size=6>\<martin.soyland@stv.uio.no\></font></br>

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
# setwd("./undervisningsmateriell/forelesninger/uke_02/")
# rsconnect::writeManifest("./")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(class.source = "code-bg")
refs <- bibtex::read.bib("../../../referanser/stv2022.bib")

library(rvest)
library(tidyverse)
library(tweetrmd)
```


# Disposisjon

1. Strukturerte datasett
2. Semistrukturerte data (APIer)
3. Ustrukturerte data
    - Skraping (med eksempler)
    - Kravling (med eksempler)
    - Fallgruver
4. Kaosdata (Høyland & Søyland)
    - En historie om OCR
6. Oppsummering
5. Oppgave


# Strukturerte datasett

<div class='left' style='float:left;width:45%'>
![European Values Study](./evs_crop.png){width=40%} </br>
![Varieties of Democracy](./vdem_crop.png){width=40%} </br>
![Stortingscrape](./ss_crop.png){width=40%} </br>
</div>

<div class='right' style='float:right;width:45%'>
![Fivethithyeight](./fte.png){width=40%} </br>
![Parlgov](./parlgov.png){width=40%} </br>
![Quality of Government](./qog_transp.png){width=40%} </br>
</div>

## Strukturerte datasett

- Datasett som er klar *out of the box*
- oftest 2-dimensjonale: `data[rader, kolonner]`
- ... men mange trenger fortsatt preprosessering

. . .

```{r struct_ex, echo=-1}
set.seed(984984)
tibble(stortingscrape::cases$root) %>%      # Henter ut data fra stortingscrape
  select(id, status, type, title_short) %>% # Trekker ut relevante variabler
  slice_sample(n = 5)                       # Viser 5 tilfeldige enheter

```

## {data-background="./file_formats.png" }

## Vanlige dataformat


| Format                                                                       | Står for                   | R-funksjon                                    |
|:------|:---------------------------|:----------------------------------------------|
| [.csv](https://fileinfo.com/extension/csv)                                   | Comma Separated Values     | `read.csv` // `readr::read_csv()`             |
| [.txt](https://fileinfo.com/extension/txt)                                   | Text                       | `readLines()` // `textreadr::read_document()` |
| [.xlsx](https://fileinfo.com/extension/xlsx)                                 | Excel                      | `readxl::read_xlsx()`                         |
| [.html](https://fileinfo.com/extension/html)                                 | HyperText Markup Language  | `rvest::read:html()`                          |
| [.xml](https://fileinfo.com/extension/xml)                                   | Extensible Markup Language | `rvest::read:html()`                          |
| [.json](https://fileinfo.com/extension/json)                                 | JavaScript Object Notation | `jsonlite::read_json()`                       |
| [.dta](https://www.loc.gov/preservation/digital/formats/fdd/fdd000471.shtml) | Stata                      | `haven::read_dta()`                           |
| [.sav](https://fileinfo.com/extension/sav#spss_data_file)                    | SPSS                       | `haven::read_sav()`                           |
| ...                                                                          | Og tusen andre formater... | ...med egne R-pakker og funksjoner            |


# Semistrukturerte data (APIer)

Application Programming Interface:

. . .

![API](./api_negate.png){width=50%}

. . .

Mer i forelesning <p style='color:#5B28D4'>[05] Bruke API (Stortinget) (uke 38)</p>

---

[Tilfeldig side fra Statens vegvesen](https://psi.norge.no/los/ord/parkering-og-hvileplasser)

</br></br>

```{bash xml_eks, echo=FALSE}
xmllint --encode utf8 --format xml_eks.xml
```



# Ustrukturerte data

</br> </br> 
![](meme-data-data-everywhere.png){width=55%}

## Skraping vs. kravling

<!-- - Skraping (*web scraping*) er spesifikk -->
<!--     - vi vet hva vi vil ha, og henter bare det -->
<!-- - Kravling (*web crawling*) er generell -->
<!--     - vi vet vi vil ha alt, men ikke nødvendigvis hva alt er -->

. . .

<div class='left' style='float:left;width:50%'>
![](scrape.png){width=80%}
</div>

. . .

<div class='right' style='float:right;width:50%'>
![](crawl.png){width=70%}
</div>


# Skraping


```{r tweet_regjeringen, out.width=".75\textwidth"}
tweet_screenshot(
  tweet_url("Regjeringen", "1526207283872641025"),
  theme = "dark",
  align = "center",
  hide_media = TRUE
)
```



## Skraping

- Eksempelhypotese:

. . .

> Skandinaviske land blir nevnt oftere enn andre land i NOUer fra Utenriksdepartementet

1. Utvalg/univers: aller NOUer fra UD
2. Datakilde: [regjeringen.no](https://www.regjeringen.no/no/dokument/nou-ar/id1767/?ownerid=833)
3. ~~Fremgangsmåte 1: Laste ned alle 7 dokumentene manuelt~~
4. <font color="#ffa200">Fremgangsmåte 2: Lage en scraper som laster ned og strukturerer dokumentene</font>

## Steg 1: Finne data

- <a href="https://www.regjeringen.no/" target="_blank">Point and click på regjeringen.no</a>
- *Inspect*
- Finn *selector*
- Skriv kode og test med én link

---

```{r nou_base, eval=-c(9,10), echo=TRUE}

library(rvest)
library(stringr)

# rot-url for regjeringen.no
base_url <- "https://www.regjeringen.no"

# Laster ned siden med alle NOUer fra UD (atm 7 stk)
str_c(base_url, "no/dokument/nou-ar/id1767/?ownerid=833") %>% 
  download.file(., destfile = "./scrape/base.html")

# Trekker ut linkene til hver NOU
nou_links <- read_html("./scrape/base.html") %>% 
  html_elements("li > h2 > a[data-hitid]") %>% 
  html_attr("href")

paste0(base_url, nou_links)
```

## Steg 2: Laste ned forside for alle NOUer

- <a href="https://www.regjeringen.no/no/dokumenter/nou-2016-8/id2503028/" target="_blank">Bruk én lenke som testobjekt først</a>
- Når det virker: Lag en løkke som laster ned alle NOUene sin forside
- Hvis det krasjer på 1 eller flere sider, inspiser og fiks koden

---

```{r nou_forside, eval=FALSE, echo=TRUE}

# Laster ned forsiden til hver NOU
for(i in nou_links){
  
  # Trekker ut dokument-id
  tmp_id <- str_extract(i, "id[0-9]+")
  
  # Laster ned forsiden til NOU i
  str_c(base_url, i) %>% 
    download.file(., destfile = str_c( "./scrape/nou_forside/", 
                                       tmp_id, 
                                       ".html"))
  
  # Legger til litt tilfeldig søvn
  Sys.sleep(2 + abs(rnorm(1, 0)))
  
  # Printer en beskjed til console om at i nå er ferdig
  message(str_c(
    "Ferdig med: ",
    tmp_id,
    "\n"
  ))
  
}

```

---

```{r sjekke_at_filer_er_der, echo=TRUE, eval=TRUE}

# Lister opp forsidene vi har lagret
list.files("./scrape/nou_forside/")

# Les inn hver forsidefil med read_html()
nou_forsider <- lapply(
  list.files("./scrape/nou_forside/", full.names = TRUE), 
  read_html
)

# Viser første listeelement
nou_forsider[[1]]

```

## Steg 3: Laste ned `.pdf` for alle NOUene

- Se på forsiden til én NOU og identifiser nedlastingsknappen
- *Inspect* -> *Copy selector* -> Rediger og test
- Skriv kode for å trekke ut lenkene
- Last ned `.pdf`-filene

---

```{r nou_pdf_links, echo=TRUE, eval=TRUE}

# Prosesserer hvert listeelement
nou_pdf_links <- sapply(nou_forsider, function(x){
  
  tmp_content_link <- x %>%       # Trekk ut listeelement x
    html_elements("a[title]") %>% # Trekk ut html nodene "a" som har egenskapen "title"
    html_attr("href")             # Trekk ut teksten til egenskapen "href"
  
  # Trekk ut bare de linkene som inneholder ".pdf" på slutten av strengen
  tmp_content_link %>% 
    magrittr::extract(str_detect(., "\\.pdf$"))
  
  
})

# Vis linkene
nou_pdf_links
```

---

```{r nou_pdf_dl, echo=TRUE, eval=FALSE}
# Laster ned pdf til hver NOU
for(i in nou_pdf_links){
  
  # Trekker ut dokument-id
  tmp_id <- str_extract(i, "nou(.*?)\\.pdf$")
  
  # Laster ned forsiden til NOU i
  str_c(base_url, i) %>% 
    download.file(., destfile = str_c("./scrape/nou_pdf/", 
                                      tmp_id))
  # Legger til litt tilfeldig søvn
  Sys.sleep(2 + abs(rnorm(1, 0)))
  
  # Printer en beskjed til console om at i nå er ferdig
  message(str_c(
    "Ferdig med: ",
    tmp_id,
    "\n"
  ))
  
}

# Lister opp filene
list.files("./scrape/nou_pdf/")
```

```{r juksechunk, echo=FALSE, eval=TRUE}
list.files("./scrape/nou_pdf/")
```

## Steg 4: Laste inn `.pdf`-filene

- Her bruker vi `textreadr::read_pdf()`

. . .

```{r nou_lese_pdf, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(textreadr)

# Lager objekt for filbane til alle pdfene
nou_pdfer <-  list.files("./scrape/nou_pdf/", full.names = TRUE)

# Gå gjennom hver fil og...
nou_tekst <- lapply(nou_pdfer, function(x){
  
  # ... les pdfen og gjør det om til en tibble
  read_pdf(x) %>% tibble()
  
})

# Viser de første 4 radene på listeelement 1
nou_tekst[[1]] %>% 
  head(., 4)
```

## Steg 5: Konvertere til `.txt`

```{r nou_txt, eval=FALSE, echo=TRUE}

# Fra 1 til 7 (antall NOUer) ...
lapply(1:length(nou_tekst), function(x){
  
  tmp_tekst <- nou_tekst[[x]] %>%            # ...trekk ut listeelement x (1:7)
    summarize(tekst = str_c(text,            # ...og slå sammen teksten til én tekstbolk
                            collapse = " "))
  
  
  txt_file_out <- nou_pdfer[x] %>%           # Trekk ut filbane for x
    str_extract("nou[0-9]+(.*?)\\.pdf") %>%  # Trekk ut bare filnavn for filbanen til x
    str_remove("\\.pdf")                     # Fjern ".pdf" fra filbanen til x
  
  
  writeLines(tmp_tekst$tekst,            # Skriv teksten til en .txt fil
             str_c("./scrape/nou_txt/",  # som skal lagres i mappen ./scrape/nou_txt/
                   txt_file_out,         # med navnet vi laget over
                   ".txt"))              # og .txt suffix
  
})
```

## Steg 6: Neste forelesning ...

... men husk hypotesen:

> Skandinaviske land blir nevnt oftere enn andre land i NOUer fra Utenriksdepartementet

---

```{r nou_analyse, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
library(tidytext)

# Hentet fra: https://no.wikipedia.org/wiki/Liste_over_land_etter_areal
countries <- rvest::read_html("./scrape/countries.html") %>% 
  html_elements("table") %>% # Trekker ut bare tabell
  html_table() %>%           # Gjør om til en tibble-liste
  bind_rows() %>%            # og gjør om fra liste til bare tibble
  pull(Land) %>%             # Trekker ut variabelen "Land", bare
  tolower()                  # Gjør om til små bokstaver

# Lager en vektor med filbane til alle .txt-filene
txt_filer <- list.files("./scrape/nou_txt/", full.names = TRUE)

# Leser inn alle .txt-filene
nouer <- lapply(txt_filer, function(x){
  readLines(x) %>%         # Leser fil x
    str_c(collapse = " ")  # Kollapser alle linjer til én bolk med tekst
})

# Lager endelig datasett...
nou_data <- tibble(id = str_extract(txt_filer, "nou[0-9]+(.*?)pdf[as]"),
                   tekst = unlist(nouer)) %>%       # ...der id er nou-id
  unnest_tokens(token, tekst) %>%                   # ...teksten splittes opp i enkeltord
  filter(token %in% countries) %>%                  # ...tar med bare ord som er i landlista
  filter(token %in% c("norge", "man") == FALSE) %>% # ...fjerner norge og "man"
  filter(str_detect(token, "^eu$") == FALSE) %>%    # ...fjerner EU
  count(token)                                      # ...og teller antall ganger land er nevnt


# Lager en vektor for skandinaviske land
skandinavia <- c("danmark",
                 "sverige",
                 "færøyene",
                 "finland",
                 "island")

# Lager ny variabel som tar verdien "ja" hvis et land er skandinavisk
# og "nei" hvis ikke
nou_data$skandinavia <- ifelse(nou_data$token %in% skandinavia, "ja", "nei") %>% 
  factor(., levels = c("nei", "ja"))

library(ggwordcloud)

nou_data %>% 
  ggplot(., aes(label = token,                         # Lager plot med token som label
                size = n,                              # n som størrelse
                color = skandinavia)) +                # og ja/nei på skand. som farge
  geom_text_wordcloud_area()+                          # Lager ordsky geom
  scale_size_area(max_size = 40) +                     # Justerer størrelse på ordskyen
  scale_color_manual(values = c("darkcyan", "red")) +  # Setter farge for ja/nei på skand.land
  ggdark::dark_theme_void()                            # Skifter til mørkt tema


```

---

OBS!

```{r skand_reg, echo=FALSE, results='asis'}
(lm(n ~ skandinavia, data = nou_data)) %>%                   # Lager en regresjon med n som AV og skand. som UV
  stargazer::stargazer(.,                                    # Lager tabell av regresjonen
                       type = "html",                        # som skal komme ut i .html
                       keep.stat = c("n", "adj.rsq"),        # Tar vare på ant. enheter og rsq av statistikker
                       covariate.labels = c("Skand. land",   # Skifter navn på koeffisientene i regresjonen
                                            "Konstantledd"))
```


# Kravling


<div class='left' style='float:left;width:50%'>
![Edderkopp](./edderkopp.gif){width=70%}
</div>

. . .

<div class='right' style='float:right;width:50%'>
- La oss kravle litt rundt på [Virksomme ord](http://virksommeord.no)
- Dette vil ta lang tid! Husk å teste først
- Se på "Taler -- kronologisk"
- Gå inn på en tale og legg merke til linken `/tale/xxxx/`
- Setter kravleren til å bare lagre disse
</div>



---

Ca. 4 timer kjøretid
```{r en_kravler, eval=FALSE, echo=TRUE}

# Laster inn pakke for kravling
library(Rcrawler)


Rcrawler("http://virksommeord.no/", # Nettsiden vi skal kravle
         DIR = "./crawl",           # mappen vi lagrer filene i
         no_cores = 4,              # kjerner for å prosessere data
         dataUrlfilter = "/tale/",  # subset filter for kravling
         RequestsDelay = 2 + abs(rnorm(1)))


```



---

```{r juksechunk2, eval=TRUE, echo=TRUE}
# Lager en vektor med alle filnavn
virkord_filer <- list.files("./crawl/virksommeord.no-101413", 
                            full.names = TRUE)

# Viser de første filene i vektoren...
head(virkord_filer)

# Og lengden på vektoren
length(virkord_filer)

```

---

### Kravlingens etterarbeid

```{r, eval=FALSE, echo = TRUE}

# Leser inn alle filene fra kravlingen
virkord_html <- lapply(virkord_filer, rvest::read_html)

# Går gjennom alle filene med preprosessering
virkord_data <- lapply(1:length(virkord_html), function(x){
  
  tmp_tekst <- virkord_html[[x]] %>%               
    html_elements("div[class='document'] > p") %>% # Trekker ut elementene <div class="document"> etterfulgt av <p>
    html_text()                                    # Konverterer til tekst
  
  if(identical(character(), tmp_tekst)){           # Hvis teksten er tom...
    tmp_tekst <- virkord_html[[x]] %>%             #
      html_elements("tr[valign='top']") %>%        # ...trekker jeg ut <tr valign="top>
      html_text()                                  # og gjør om til tekst
  }
  
  if(identical(character(), tmp_tekst)){              # Hvis teksten fortsatt er tom...
    tmp_tekst <- virkord_html[[x]] %>%                #
      html_elements("div[class='document'] > h3") %>% # ...trekker jeg ut elementene <div class="document"> etterfulgt av <h3>
      html_text() %>%                                 # ...gjør om til tekst
      str_split(., "\\n") %>%                         # ...splitter opp i linjer
      unlist()                                        # ... og konverterer fra liste til vektor
  }
  
  # Trekker ut forfatternavn
  tmp_forfatter <- virkord_html[[x]] %>% 
    html_elements("div[class='tale-header'] > ul[class='byline'] > li > a") %>% 
    html_text() %>% 
    str_replace_all(., "\\s+", " ") %>% 
    .[1]
  
  # Trekker ut link til forfatterside
  tmp_forfatter_link <- virkord_html[[x]] %>% 
    html_elements("div[class='tale-header'] > ul[class='byline'] > li > a") %>% 
    html_attr("href") %>% 
    .[which(str_detect(., "person"))]
  
  # Trekker ut tittel
  tmp_tittel <- virkord_html[[x]] %>% 
    html_elements("div[class='tale-header'] > h1") %>% 
    html_text() %>% 
    str_replace_all(., "\\s+", " ")
  
  # Setter alt sammen til en tibble
  tmp_data <- tibble(
    tittel = tmp_tittel,
    forfatter = tmp_forfatter,
    forfatter_link = tmp_forfatter_link,
    avsnitt = 1:length(tmp_tekst),
    tekst = tmp_tekst
  )
  
  return(tmp_data)
})

# Binder så sammen alle tibbles i listen "virkord_data"
# Til å være én tibble
virkord <- bind_rows(virkord_data)


# save(virkord, file = "./crawl/virksommeord.rda")
```

```{r load_virkdata, echo = FALSE, eval=TRUE}
load("./crawl/virksommeord.rda")
```

---

```{r vise_virkdata, echo=TRUE, eval=TRUE}
# Viser de 6 øverste radene i datasettet
virkord %>% sample_n(6)
```

# Fallgruver

</br></br>

![Fallguys](./fallguys.gif){width=80%}

## Front-end vs. back-end

- [Regjeringensarkiv](https://www.regjeringen.no/no/om-regjeringa/tidligere-regjeringer-og-historie/sok-i-regjeringer-siden-1814/id2578015/)
    - regjeringen.no har ikke åpen API
    - men har en back-end API som henter data for oss som gjør siden lesbar
- Se på *Network* i *Inspect*
    - data henter [.json-fil](https://www.regjeringen.no/api/RegjeringDataApi/GetRegjeringer?language=no&term=&period=&page=1) back-end
    - merk: `"Page":1`, `"Hits":20`, `"TotalHits":71`, osv


## Overlaste serveren (timeout){data-background="./504_negate.png"}


## Lagring {data-background="./win_disk.png" data-background-size=25% data-background-position="left"}

- Alltid lagre lokalt!
    - slipper å hente samme side flere ganger
    - reproduserbarhet -> nettsider endrer seg
    - `download.file()` er din venn

## robots.txt

- `robots.txt` eksisterer på de fleste nettsider
    - beskriver hva som er lov og ikke lov å skrape

. . .

```{r robots, eval=-1}
download.file("https://vg.no/robots.txt", destfile = "./scrape/vg_robots.txt")

readLines("./scrape/vg_robots.txt")

```

# Kaosdata: En historie om OCR

Høyland & Søyland (2019):

- **Hypotese:** 
    - Mer partisentererte valgsystemer gir mer "partisnakk"
- **Data:**
    - Stortingstaler fra 1910-1931
- **Fremgangsmåte:** 
    - Kvantitativ emneanalyse på tvers av den norske valgreformen i 1919
- **Funn:** 
    - Ja, valgreform endrer atferd
- **Implikasjon:** 
    - Valgsystemet endrer representanters vekting av å følge partilinjen mot å 
    representere sitt valgdistrikt

## Samle data

- Skannede dokumenter fra 1910-1931
- Tigjengelig hos Nasjonalbiblioteket
    - Last ned med "søk og klikk"

. . .

![Image Title](./not_noisy.png){width=70%}

---

## Preprosessering -- ImageMagick

```{bash imagemagick, eval = FALSE, echo = TRUE}
convert -density 300x300 -shave 10x100;
  not_noisy.png not_noisy_clean1.png

convert not_noisy_clean1.png -normalize ;
  not_noisy_clean2.png

convert not_noisy_clean2.png ;
  -connected-components 4 -threshold 0 -negate;
  not_noisy_clean3.png

convert not_noisy_clean2.png;
  -define connected-components:area-threshold=15;
  -connected-components 4 -threshold 0 -negate; 
  not_noisy_clean4.png

convert not_noisy_clean3.png not_noisy_clean4.png;
  -compose minus -composite;
  not_noisy_clean5.png

convert not_noisy_clean2.png;
  \( -clone 0 -negate -fill white -colorize 100% \) ;
  not_noisy_clean5.png -compose Blend -composite;
  not_noisy_clean6.png

convert not_noisy_clean6.png; 
  -fill black -opaque "#FF00FF" -morphology Erode Disk:0.5; 
  not_noisy_cleaned.png
```

. . .

![Original](./not_noisy.png){width=40%}
![Clean](./not_noisy_cleaned.png){width=40%}

---

## Preprosessering -- Bilde til tekst

![Clean](./not_noisy_cleaned.png){width=70%}

```{bash tesseract, eval = FALSE, echo = TRUE}
# https://github.com/tesseract-ocr/tesseract
tesseract not_noisy_cleaned.png tekst
head -n 10 tekst.txt
```

. . .

```{r show_ocr_text, echo = FALSE, eval = TRUE, warning = FALSE}
readLines("./tekst.txt", n = 7)
```



---

## Preprosessering -- Når ting går galt

- Valg tatt i skanneprosessen kan ha konsekvenser 
- _Grayscale_ vs. sort/hvit
- Linjebrytning av ord med bindestrek

. . .

![Noisy](./noisy.png){width=20%}
![Noisy cleaned](./noisy_cleaned.png){width=20%}

. . .

```{r show_noisy_ocr_text, echo = FALSE, eval = TRUE, warning = FALSE}
readLines("./tekst_noisy.txt", n = 5)
```

---

## Preprosessering -- Tekst til tall (BoW)


```{r bow_ex, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}

samp_tekst <- read_csv("st_debates_meta.csv", show_col_types = FALSE)

set.seed(4093)

samp_tekst[sample(1:nrow(samp_tekst), 6), c("speech_id", "name_fixed", "speech")]
```

. . .

```{r bow_ex2, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
samp_tekst <- samp_tekst[sample(1:nrow(samp_tekst), 100), ]

samp_tekst <- samp_tekst %>%
  unnest_tokens(output = "token", input = "speech") %>%
  filter(grepl("[0-9]", token) == FALSE) %>% 
  group_by(speech_id) %>% count(token) %>% 
  cast_dfm(., document = speech_id, term = token, value = n)

as.matrix(samp_tekst[1:6, c("det", "har", "jord", "lærere", "behandling", "park")])

```


---

## Preprosessering -- Hva skal med?

- Stoppord
- Bøyning 
    - full form vs. stemming vs. lemma
    - kjærlighet vs. kjær vs. kjærlighet
- Rekkefølge på ord 
    - unigram, bigram, trigram, osv
- Nynorsk vs. bokmål?
- Kontekstuelle (ikke-språklige) variabler

## Modellering -- Emneanalyse

- Emneanalyse (topic modelling)
    - Emner defineres av ords sameksistens
    - Satt antall emner -- abstraksjonsnivå
    - Induktiv tolkning -- mye lesing
- Structural Topic Model (STM)
    - Estimere effekt av variabler på emnestruktur
    - Her med reform og stortingssesjon


## Modellering -- Resultat

![Topic Examples](./topics_examples.png){width=90%}

---

![Effect](./reform_effect.png){width=90%}


# Oppsummering

- Strukturerte data :: kaosdata
  - *Out of the box*
  - APIer
  - Skraping
  - Kravling
  - OCR
- Fallgruver
  - Undersøk om back-end finnes
  - Ikke overlast serveren
  - Lagre nedlastede sidene
  - Se på `robots.txt`


# Oppgave [1]

1. Skissér en hypotese basert på eksisterende teorier
    - Husk iterativ prosess -- ikke overtenk!
2. Finn en datakilde du tenker kan brukes til å svare på hypotesen din
    - Pass på å ikke ta vann over hodet!
3. Hent og strukturer data
    - Om det er mye data, start med å strukturere deler av data
4. Gi en kort beskrivelse av hvordan dataene ble fanget og hvordan de er strukturert
    - Ta gjerne med litt om problemer som evt oppstod

# Neste forelesning(er)

</br></br></br>

<p style='color:#D13F11;font-weight:bold'>Forbehandling av tekst 1 (uke 36)</p>

. . .

<p style='color:#D13F11;font-weight:bold'>Forbehandling av tekst 2 (uke 37)</p>

. . .

<p style='color:#5B28D4;font-weight:bold'>Bruke API (Stortinget) (uke 38)</p>


```{r writeScript, echo=FALSE, eval=FALSE}
knitr::purl("./uke_02_anskaff.Rmd", "./uke_02_anskaff.R", documentation = 1)
```